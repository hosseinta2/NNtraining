#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Nov 10 15:22:54 2023

@author: hosseintaheri
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun 27 13:09:50 2023

@author: hosseintaheri
"""
"""
See the rest of the video to learn about CNN
"""

import torch
#print(torch.__version__)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from torch import nn

import torchvision
from torchvision import transforms
from torchvision import datasets
from torchvision.transforms import ToTensor
from numpy import linalg as la
from torch.utils.data import DataLoader # dividing into batchs

# importing and downloading traing dataset
train_data0 = datasets.CIFAR100(root="data", # where to dowload 
                                   train= True, # training set?
                                   download= True,# download?
                                   transform = torchvision.transforms.ToTensor(), # transforming the data
                                   target_transform= None)# transforming the labels

test_data0 = datasets.CIFAR100(root="data", # where to dowload 
                                   train= False, # training set?
                                   download= True,# download?
                                   transform = torchvision.transforms.ToTensor(), # transforming the data
                                   target_transform= None)# transforming the labels

train_data = []
test_data =[]
for data in train_data0:
    x,y = data
    if y==0 or y==1 :
        train_data.append(data)
for data in test_data0:
    x,y = data
    if y==0 or y==1 :
        test_data.append(data)
        
#idx = (train_data.targets==1) | (train_data.targets==2) ## the next three lines only worked for fashionmnist not cifar10
#train_data.targets =train_data.targets[idx]
#train_data.data = train_data.data[idx]

#idx = (test_data.targets==1) | (test_data.targets==2) 
#test_data.targets =test_data.targets[idx]
#test_data.data = test_data.data[idx]

#test_data.targets=torch.tensor(test_data.targets)
#train_data.targets=torch.tensor(train_data.targets)

#plt.imshow(image.squeeze(),cmap="gray")

train_dataloader = DataLoader(dataset=train_data, batch_size=len(train_data), shuffle=True)
test_dataloader = DataLoader(dataset=test_data, batch_size=len(test_data), shuffle=True)

# # buildign model with nn.sequential
# class fashionmnistCNNmodel(nn.Module):
#     def __init__(self, 
#                  input_shape:int,
#                  hidden_units: int,
#                  output_shape: int):
#         super().__init__()
#         self.layer_stack = nn.Sequential(
#             nn.Flatten(),
#             nn.Linear(in_features=input_shape,
#                       out_features = hidden_units),
#             nn.Linear(in_features=hidden_units,out_features=output_shape)
#             )
        
#     def forward(self, x):
#         return self.layer_stack(x)
    
# building from old-fasghioned ways
class fashionmnistCNNmodel1(nn.Module):
    def __init__(self,in_features,hidden_units1,hidden_units2,out_features):
        super().__init__()
        self.layer1 = nn.Linear(in_features, hidden_units1)
        self.layer2 = nn.Linear(hidden_units1, hidden_units2)
        self.layer3 = nn.Linear(hidden_units2, out_features)
        self.ReLU   = nn.Softplus()
        self.flatten= nn.Flatten()
        self.m = hidden_units1**0.5 # sqrt(m)
        torch.nn.init.normal_(self.layer1.weight, 
                        mean=0, std=1)
        torch.nn.init.normal_(self.layer2.weight, 
                        mean=0, std=1)
        torch.nn.init.normal_(self.layer3.weight, 
                        mean=0, std=1)
    
        
    def forward(self, x):
        return self.layer3(self.ReLU(self.layer2( self.ReLU( self.layer1( self.flatten(x)) )/self.m) )/self.m )
    
def accuracy_fn (y,y_pred):
    num_correct = torch.eq(y,y_pred).sum().item()
    acc = num_correct / len(y_pred) 
    return acc

### 
## hu1 = 20 --> step_size = 0.03
## hu1= 50 -->step_size = 0.02
## hu1 =100 --->step_size = .02
test_acc_m = []
test_loss_m=[]
test_error_m = []
weight_diff_m = []
weight_diff2_m = []
train_loss_m = []
vec = [20,100]


for hu1 in vec: # numeder of hidden units
    
    model0 = fashionmnistCNNmodel1(in_features=3*32*32, hidden_units1=hu1, hidden_units2 = hu1, out_features=2)
    
    # accuracy function

    
    # loss and optimizer
    loss_fn = nn.CrossEntropyLoss()
    step_size = 0.015
    optimizer = torch.optim.SGD(params = model0.parameters(), lr = step_size)
    
    T = 500 #num of iterations
    train_loss = []
    test_acc=0
    test_loss = []
    test_error = []
    nnweights_t = []
    for epoch in range(T):
      
    
    # implementing batch SGD
      for X_train,y_train in train_dataloader:# number of steps
        params=model0.parameters()
        nnweights = []
        for i,data in enumerate(params):
            if i==0 or i==2:
                listed = data.tolist()
                flat_list = [item for sublist in listed for item in sublist]
                nnweights = nnweights+flat_list
            if i==1:
                listed = data.tolist()
                nnweights = nnweights+listed
                
        nnweights_t.append(nnweights)
        
             
        model0.train() # put the model in training mode (this is the default settings)
        y_pred = model0(X_train)# forward pass
        loss = loss_fn(y_pred,y_train) #calculate the loss
        train_loss.append(loss)
        optimizer.zero_grad() # zero the gradient of optimizer
        loss.backward() # backpropagate on the loss
        optimizer.step() # one step of SGD
        #model0.eval() # puts the model in evaluation mode
        if epoch%50==0:
           print(epoch)
      with torch.inference_mode(): # evaluating the model
        for X_test,y_test in test_dataloader:
           test_preds = model0(X_test)
          
           
           test = loss_fn(test_preds,y_test)
           test_loss.append(test)
           a = test_preds.tolist()
           b  = [a[i].index(max(a[i])) for i in range(len(a))]
           test_preds = torch.tensor(b)
           test_error.append(1-accuracy_fn(y_test, test_preds))
      #print (f"train loss = {train_loss/len(train_dataloader)}, test accuracy = {test_loss/len(test_dataloader)}") 
    train_loss_list = []
    for i in train_loss:
        train_loss_list.append(i.item())
    
    #plt.yscale("log")
    diff = []
    diff2 = []
    init = np.array(nnweights_t[0])
    
    for i in range(T):
        wi = np.array(nnweights_t[i])
        diff.append(la.norm(wi-init))
        diff2.append(la.norm(wi))
    weight_diff_m.append(diff)
    weight_diff2_m.append(diff2)
    test_acc_m.append(test_loss)
    test_error_m.append(test_error)
    train_loss_m.append(train_loss_list)

fs = 14
fig1 = plt.figure("Figure 1")
fig1, ax = plt.subplots()
ax.set_facecolor('ghostwhite')
for i in range(len(vec)):
    
    hold1 = [x for x in test_acc_m[i]]
    #test_loss_m.append(hold) 
    plt.plot (range(5,T), hold1[5:], label ="Test Loss, width=%d" %vec[i] , linewidth = 2.5)

#plt.plot (range(T), train_loss_list, label ="train loss, m=%d" %hu1 , linewidth = 0.8)
plt.gca().set_prop_cycle(None) # reset the colors in plot
for i in range(len(vec)):
    
    hold2 = [1-x for x in test_error_m[i]]
    #test_loss_m.append(hold) 
    plt.plot (range(5,T), hold2[5:], label ="Test Error, width=%d" %vec[i] , linestyle='--', linewidth = 2.5)

#plt.plot (range(T), train_loss_list, label ="train loss, m=%d" %hu1 , linewidth = 0.8)



# for i in range(len(vec)):
#     gengap = [sum(train_loss_m[i][:j])/len(train_data) for j in range(1,len(train_loss_m[i])+1)]
#     gengap1 = [gengap[j] + train_loss_m[i][j] for j in range(len(train_loss_m[i]))]
#     #plt.plot (range(T), gengap1, label ="stability bound, m=%d" %vec[i] , linestyle='--', linewidth = 0.8)

# #plt.plot (range(T), train_loss_list, label ="train loss, m=%d" %hu1 , linewidth = 0.8)
plt.xlabel("t",fontsize=fs)
#plt.ylabel("test loss",fontsize=fs)

plt.yticks(np.arange(0, 1, step=10**(-1)))
plt.yscale("log")
plt.legend()

fig1.savefig('fig2-2.eps', format='eps', dpi=1200)




fig2 = plt.figure("Figure 2")
fig2, ax = plt.subplots()
ax.set_facecolor('ghostwhite')
for i in range(len(vec)):
    plt.plot (range(T), weight_diff_m[i], label ="width=%d" %vec[i] , linewidth = 2.5)
plt.gca().set_prop_cycle(None) # reset the colors in plot
#for i in range(len(vec)):
    #plt.plot (range(T), weight_diff2_m[i], label ="$\| w_t \|$, width=%d" %vec[i] , linestyle='--', linewidth =2.5)
plt.xlabel("t",fontsize=fs)
plt.ylabel("$\| w_t - w_0 \|$",fontsize = fs)
plt.legend()

fig2.savefig('fig2-1.eps', format='eps', dpi=1200)

fig3 = plt.figure("Figure 3")
fig3, ax = plt.subplots()
ax.set_facecolor('ghostwhite')
for i in range(len(vec)):

    plt.plot (range(10,T), train_loss_m[i][10:], label ="m=%d" %vec[i] , linewidth = 2.5)

#plt.yscale("log")
plt.xlabel("t",fontsize=fs)
plt.ylabel("Train Loss",fontsize=fs)
#plt.legend()
fig3.savefig('fig2-3.eps', format='eps', dpi=1200)



fig4 = plt.figure("Figure 4")
fig4, ax = plt.subplots()
ax.set_facecolor('ghostwhite')
# for i in range(len(vec)):
#     gen_loss= [test_loss_m[i][j] - train_loss_m[i][j] for j in range(len(test_loss_m[i]))]
#     plt.plot (range(T), gen_loss, label ="m=%d" %vec[i] , linewidth = 0.8)

# #plt.plot (range(T), train_loss_list, label ="train loss, m=%d" %hu1 , linewidth = 0.8)
# plt.gca().set_prop_cycle(None) # reset the colors in plot

for i in range(len(vec)):
    gengap = [sum(train_loss_m[i][:j])/len(train_data) for j in range(1,len(train_loss_m[i])+1)]
    plt.plot (range(T), gengap, label ="stability bound, m=%d" %vec[i] , linestyle='--', linewidth =2.5)

plt.xlabel("t",fontsize=fs)
plt.ylabel("Gen. bound (stability)",fontsize=fs)
fig4.savefig('fig2-4.eps', format='eps', dpi=1200)
